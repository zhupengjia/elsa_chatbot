%YAML 1.2
---
#app setting

skill:
    name: cornell
    wrapper: GenerativeResponse

tokenizer:
    bert_model_name: bert-base-uncased
    do_lower_case: 1

reader:
    wrapper: ReaderCornell
    train_data: /home/pzhu/data/dialog/cornell_corpus
    max_seq_len: 20

model:
    device: "cuda:0"
    model_type: "transformer"
    pretrained_embedding: data/word2vec/en/bert-base-uncased.h5py
    encoder_hidden_layers: 3
    encoder_hidden_size: 768
    encoder_intermediate_size: 1024
    encoder_attention_heads: 3
    encoder_freeze: False
    dropout: 0.1
    decoder_hidden_layers: 3
    decoder_attention_heads: 3
    decoder_hidden_size: 1024
    learning_rate: 0.0001
    weight_decay: 0
    momentum: 0.9
    optimizer: "adam"
    epochs: 10000
    batch_size: 30
    num_workers: 1
    saved_model: 'data/cornell/model_transformer.pt'
    save_per_epoch: 1

reinforcement:
    learning_rate: 0.0001
    maxloop: 20
    discount: 0.95

logger:
    appname: cornell
    loglevel_console:  20
    loglevel_file: 0
    logfile: data/cornell/train.log

